{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4FmFHkMzp3l"
   },
   "source": [
    "## Importing Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qJlAaNYsgGBc",
    "outputId": "f6c05099-ed67-4511-fda3-540579ccaebe"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet cython\n",
    "\n",
    "!pip install --quiet numpy==1.24.3 scipy==1.10.1\n",
    "\n",
    "!pip install --no-binary gensim gensim nltk\n",
    "\n",
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8539,
     "status": "ok",
     "timestamp": 1750096623114,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "Gcf9ew6xgNAT",
    "outputId": "87e96947-42e4-4aa3-b30b-a581b75c32ae"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "from collections import OrderedDict\n",
    "import csv\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from google.colab import drive\n",
    "import gensim.downloader as api\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel, LdaModel, LdaMulticore\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1750096674888,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "_IybAUWAgU05",
    "outputId": "e8aba83d-33e8-42d1-9b1e-e786dc789701"
   },
   "outputs": [],
   "source": [
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36811,
     "status": "ok",
     "timestamp": 1750096711700,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "CFd9J43ugWS_",
    "outputId": "944d7778-4cdf-4a36-a147-57151ba19e53"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/NLP_Project/data.csv',sep='ยง')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utIhfKxlzx8i"
   },
   "source": [
    "## Text Preprocessing and BoW Corpus Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJMWFzWjgXeS"
   },
   "outputs": [],
   "source": [
    "def to_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        return ast.literal_eval(x)\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwPvwNOagY2y"
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"tokens_lemmatized\"])\n",
    "\n",
    "df[\"tokens_lemmatized\"] = df[\"tokens_lemmatized\"].apply(to_list)\n",
    "\n",
    "df = df[df[\"tokens_lemmatized\"].apply(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPt8zlEbguyD"
   },
   "outputs": [],
   "source": [
    "extra_stops = {\"hi\",\"hello\",\"please\", 'regard',\"enron\", \"enrononline\", 'time', 'week', 'thank', 'today','fyi','great', 'good'}\n",
    "stops = set(stopwords.words(\"english\")) | extra_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHz4JjKtgwIw"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\",\"ner\",\"textcat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SL7K-XNgyIe"
   },
   "outputs": [],
   "source": [
    "# Process each document through spaCy then keep only alphabetic, non-stopword lemmas longer than 2 characters and of POS type NOUN, PROPN, or ADJ\n",
    "\n",
    "texts_filtered = []\n",
    "for doc in nlp.pipe(\n",
    "    [\" \".join(toks) for toks in df[\"tokens_lemmatized\"]],\n",
    "    batch_size=2000,\n",
    "    n_process=4\n",
    "):\n",
    "    filtered = [\n",
    "        tok.lemma_.lower()\n",
    "        for tok in doc\n",
    "        if (\n",
    "            tok.is_alpha\n",
    "            and tok.lemma_.lower() not in stops\n",
    "            and len(tok.lemma_) > 2\n",
    "            and tok.pos_ in (\"NOUN\",\"PROPN\",\"ADJ\")\n",
    "        )\n",
    "    ]\n",
    "    texts_filtered.append(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_WtAVmJgzuf"
   },
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# detect common two-word phrases\n",
    "bigram = Phrases(texts_filtered, min_count=5, threshold=25)\n",
    "\n",
    "# detect common three-word phrases built from our bigrams\n",
    "trigram = Phrases(bigram[texts_filtered], threshold=25)\n",
    "\n",
    "bigram_mod = Phraser(bigram)\n",
    "trigram_mod = Phraser(trigram)\n",
    "\n",
    "texts_ngrams = [\n",
    "    trigram_mod[ bigram_mod[doc] ]\n",
    "    for doc in texts_filtered\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LWqTe9OhDKa"
   },
   "outputs": [],
   "source": [
    "MAX_TOKEN_LEN   = 30\n",
    "MAX_UNDERSCORES = 2\n",
    "\n",
    "def prune_ngrams(doc):\n",
    "    pruned = []\n",
    "    for t in doc:\n",
    "        if len(t) > MAX_TOKEN_LEN:\n",
    "            continue\n",
    "        if t.count(\"_\") > MAX_UNDERSCORES:\n",
    "            continue\n",
    "        pruned.append(t)\n",
    "    return pruned\n",
    "\n",
    "bigram_mod = Phraser(bigram)\n",
    "trigram_mod = Phraser(trigram)\n",
    "\n",
    "texts_ngrams = [\n",
    "    prune_ngrams( trigram_mod[ bigram_mod[doc] ] )\n",
    "    for doc in texts_filtered\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11661,
     "status": "ok",
     "timestamp": 1750097316017,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "YvEV6Skbg1LX",
    "outputId": "bda12098-8a25-4638-93ab-4993b6ebf5ac"
   },
   "outputs": [],
   "source": [
    "# Build and filter dictionary\n",
    "id2word = corpora.Dictionary(texts_ngrams)\n",
    "id2word.filter_extremes(no_below=15, no_above=0.2, keep_n=100000)\n",
    "print(f\"Vocabolario globale: {len(id2word)} termini\")\n",
    "\n",
    "# Create BoW corpus\n",
    "corpus = [id2word.doc2bow(doc) for doc in texts_ngrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcJP3U39Fsim"
   },
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4218839,
     "status": "ok",
     "timestamp": 1750063760803,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "MK7SXXITkWmH",
    "outputId": "d35caee8-1918-4593-da38-025f170a0571"
   },
   "outputs": [],
   "source": [
    "# hyperparameter and topic grid\n",
    "ks     = [4, 5, 6, 7, 8]\n",
    "alphas = [\"symmetric\", \"asymmetric\"]\n",
    "etas   = [\"symmetric\", \"auto\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "for level in df[\"Level\"].unique():\n",
    "    idxs     = df.index[df[\"Level\"] == level].tolist()\n",
    "    texts_lvl = [texts_ngrams[i] for i in idxs]\n",
    "    corpus_lvl = [id2word.doc2bow(doc) for doc in texts_lvl]\n",
    "\n",
    "    for k, alpha, eta in itertools.product(ks, alphas, etas):\n",
    "\n",
    "        lda_tune = LdaMulticore(\n",
    "        corpus=corpus_lvl,\n",
    "        id2word=id2word,\n",
    "        num_topics=k,\n",
    "        alpha=alpha,\n",
    "        eta=eta,\n",
    "        passes=10,\n",
    "        chunksize=5000,\n",
    "        workers=max(1, multiprocessing.cpu_count()-1),\n",
    "        random_state=42\n",
    "    )\n",
    "        # compute coherence\n",
    "        coherence = CoherenceModel(\n",
    "            model=lda_tune,\n",
    "            texts=texts_lvl,\n",
    "            dictionary=id2word,\n",
    "            coherence=\"c_v\"\n",
    "        ).get_coherence()\n",
    "\n",
    "        results.append({\n",
    "            \"level\":    level,\n",
    "            \"k\":        k,\n",
    "            \"alpha\":    alpha,\n",
    "            \"eta\":      eta,\n",
    "            \"coherence\": coherence\n",
    "        })\n",
    "        print(f\"Level={level} | k={k}, alpha={alpha}, eta={eta} -> coherence={coherence:.4f}\")\n",
    "\n",
    "\n",
    "grid_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "best_params = {}\n",
    "for level, group in grid_df.groupby(\"level\"):\n",
    "    best = group.sort_values(\"coherence\", ascending=False).iloc[0]\n",
    "    best_params[level] = {\n",
    "        \"k\":     best[\"k\"],\n",
    "        \"alpha\": best[\"alpha\"],\n",
    "        \"eta\":   best[\"eta\"]\n",
    "    }\n",
    "print(\"\\nBest hyperparameters per level:\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hc910WkFzwU"
   },
   "source": [
    "## LDA Topic Modeling with Optimized Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 446180,
     "status": "ok",
     "timestamp": 1750097762208,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "op4eq0dB9-f8",
    "outputId": "6e82bcf3-6b9b-4fdb-e749-f4513ed5c33d"
   },
   "outputs": [],
   "source": [
    "# best hyperparameters from the grid search\n",
    "best_params = {\n",
    "    \"High\": {\"k\": 4, \"alpha\": \"asymmetric\", \"eta\": \"symmetric\"},\n",
    "    \"Medium\": {\"k\": 5, \"alpha\": \"asymmetric\",  \"eta\": \"symmetric\"},\n",
    "    \"Low\": {\"k\": 7, \"alpha\": \"asymmetric\",  \"eta\": \"symmetric\"},\n",
    "}\n",
    "\n",
    "level_models = OrderedDict()\n",
    "print(\"\\n Topics per Level\")\n",
    "\n",
    "for level in df[\"Level\"].unique():\n",
    "    params    = best_params.get(level, {\"k\": 4, \"alpha\": 1.0/4, \"eta\": 1.0/4})\n",
    "    num_topics = params[\"k\"]\n",
    "    alpha      = params[\"alpha\"]\n",
    "    eta        = params[\"eta\"]\n",
    "\n",
    "    # Gather the BoW vectors for the level\n",
    "    idxs      = df.index[df[\"Level\"] == level].tolist()\n",
    "    corpus_sub = [corpus[i] for i in idxs]\n",
    "\n",
    "    # Train using the level parameters\n",
    "    lda_lvl = LdaMulticore(\n",
    "        corpus=corpus_sub,\n",
    "        id2word=id2word,\n",
    "        num_topics=num_topics,\n",
    "        alpha=alpha,\n",
    "        eta=eta,\n",
    "        passes=20,\n",
    "        chunksize=5000,\n",
    "        workers=max(1, multiprocessing.cpu_count() - 1),\n",
    "        random_state=42\n",
    "    )\n",
    "    level_models[level] = lda_lvl\n",
    "\n",
    "    print(f\"\\nโ Level = {level} (n_docs = {len(idxs)})\")\n",
    "    for idx, topic in lda_lvl.print_topics(num_words=6):\n",
    "        print(f\"  Topic {idx:02d}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHWjelh3hRIR"
   },
   "outputs": [],
   "source": [
    "custom_labels = {\n",
    "    \"High\": {\n",
    "        0: \"Executive Meetings & Communications\",\n",
    "        1: \"Contract Negotiations & Team Coordination\",\n",
    "        2: \"Business & Team Operations\",\n",
    "        3: \"Market Pricing & Energy Trading\",\n",
    "    },\n",
    "    \"Medium\": {\n",
    "        0: \"Daily Team Coordination\",\n",
    "        1: \"Transaction Processing & Credit Documentation\",\n",
    "        2: \"Corporate Trading & Financial Operations\",\n",
    "        3: \"Gas Trading & Market Deals\",\n",
    "        4: \"California Power Market & Utility Operations\",\n",
    "    },\n",
    "    \"Low\": {\n",
    "        0: \"Schedule & Workflow Automation\",\n",
    "        1: \"Deal Management & Change Requests\",\n",
    "        2: \"Corporate Transactions\",\n",
    "        3: \"Regulatory Oversight & Energy Transmission\",\n",
    "        4: \"System Operations & Database Errors\",\n",
    "        5: \"California Energy Market & Electricity Pricing\",\n",
    "        6: \"Energy Company Stocks & Market Performance\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2O94GoRzfm6Z"
   },
   "outputs": [],
   "source": [
    "texts_by_level = {\n",
    "    lvl: [texts_ngrams[i] for i in df.index[df[\"Level\"] == lvl]]\n",
    "    for lvl in level_models\n",
    "}\n",
    "\n",
    "coherence_records = []\n",
    "topn = 10\n",
    "for lvl, lda in level_models.items():\n",
    "    texts_lvl = texts_by_level[lvl]\n",
    "    for t in range(lda.num_topics):\n",
    "        terms = [w for w, _ in lda.show_topic(t, topn=topn)]\n",
    "        cm = CoherenceModel(\n",
    "            topics=[terms],\n",
    "            texts=texts_lvl,\n",
    "            dictionary=id2word,\n",
    "            coherence='c_v'\n",
    "        )\n",
    "        coherence_records.append({\n",
    "            \"Level\":      lvl,\n",
    "            \"Topic\":      t,\n",
    "            \"Coherence\":  cm.get_coherence()\n",
    "        })\n",
    "\n",
    "coherence_df = pd.DataFrame(coherence_records)\n",
    "\n",
    "label_records = []\n",
    "for lvl, lda in level_models.items():\n",
    "    for t in range(lda.num_topics):\n",
    "\n",
    "        top2 = [w for w, _ in lda.show_topic(t, topn=2)]\n",
    "        label_records.append({\n",
    "            \"Level\": lvl,\n",
    "            \"Topic\": t,\n",
    "            \"Label\":  \" & \".join(top2)\n",
    "        })\n",
    "\n",
    "label_df = pd.DataFrame(label_records)\n",
    "\n",
    "dominant = []\n",
    "for i, row in df.iterrows():\n",
    "    lvl = row[\"Level\"]\n",
    "    bow = corpus[i]\n",
    "    lda = level_models[lvl]\n",
    "    top_tid, top_p = max(lda.get_document_topics(bow), key=lambda x: x[1])\n",
    "    dominant.append((i, lvl, top_tid))\n",
    "\n",
    "dominant_df = (\n",
    "    pd.DataFrame(dominant, columns=[\"doc_id\",\"Level\",\"Topic\"])\n",
    "      .groupby([\"Level\",\"Topic\"])\n",
    "      .size()\n",
    "      .reset_index(name=\"Count\")\n",
    ")\n",
    "\n",
    "result = (\n",
    "    coherence_df\n",
    "      .merge(label_df,     on=[\"Level\",\"Topic\"])\n",
    "      .merge(dominant_df,  on=[\"Level\",\"Topic\"], how=\"left\")\n",
    "      .fillna(0)\n",
    "      .sort_values([\"Level\",\"Coherence\"], ascending=[True, False])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "result[\"CustomLabel\"] = result.apply(\n",
    "    lambda r: custom_labels[r[\"Level\"]][r[\"Topic\"]],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1750097956794,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "a91ec10PirS-",
    "outputId": "06df1646-8bf8-4902-8bbb-98c06a35cbfa"
   },
   "outputs": [],
   "source": [
    "print(result[[\"Level\",\"CustomLabel\",\"Coherence\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quJqkd4c0JSi"
   },
   "source": [
    "## Topic Similarity Using Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hw4zJxKFmM3U"
   },
   "outputs": [],
   "source": [
    "tfidf_model = TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf_model[corpus]\n",
    "\n",
    "num_docs = len(corpus_tfidf)\n",
    "num_terms = len(id2word)\n",
    "\n",
    "tfidf_matrix = np.zeros((num_docs, num_terms))\n",
    "\n",
    "for doc_id, doc in enumerate(corpus_tfidf):\n",
    "    for term_id, tfidf_score in doc:\n",
    "        tfidf_matrix[doc_id, term_id] = tfidf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDvN-VJYmg-0"
   },
   "outputs": [],
   "source": [
    "# Get the dominant topic for each document\n",
    "dominant_topics = []\n",
    "for doc_bow in corpus:\n",
    "    topics = lda_lvl.get_document_topics(doc_bow)\n",
    "    dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    "    dominant_topics.append(dominant_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6uM8wM1emi-K"
   },
   "outputs": [],
   "source": [
    "unique_topics = np.unique(dominant_topics)\n",
    "num_topics = lda_lvl.num_topics\n",
    "\n",
    "ctfidf_matrix = np.zeros((num_topics, num_terms))\n",
    "\n",
    "for topic in unique_topics:\n",
    "    doc_indices = [i for i, t in enumerate(dominant_topics) if t == topic]\n",
    "\n",
    "    ctfidf_matrix[topic] = tfidf_matrix[doc_indices].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUOfegyhmlET"
   },
   "outputs": [],
   "source": [
    "feature_names = [id2word[i] for i in range(len(id2word))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77555,
     "status": "ok",
     "timestamp": 1750098139272,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "WxrR0odClieC",
    "outputId": "a09df7d1-a457-4f66-95a9-f3b1caf1d249"
   },
   "outputs": [],
   "source": [
    "def compute_cosine_coherence_ctfidf(ctfidf_matrix, feature_names, top_n=10):\n",
    "    coherence_scores = {}\n",
    "\n",
    "    for topic_idx, topic_vector in enumerate(ctfidf_matrix):\n",
    "        top_word_indices = topic_vector.argsort()[::-1][:top_n]\n",
    "        top_word_vectors = ctfidf_matrix[:, top_word_indices].T\n",
    "\n",
    "        if top_word_vectors.shape[0] < 2:\n",
    "            coherence_scores[topic_idx] = 0.0\n",
    "            continue\n",
    "\n",
    "        similarity_matrix = cosine_similarity(top_word_vectors)\n",
    "        upper_triangle = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]\n",
    "        avg_similarity = upper_triangle.mean() if upper_triangle.size else 0.0\n",
    "        coherence_scores[topic_idx] = avg_similarity\n",
    "\n",
    "    return coherence_scores\n",
    "\n",
    "level_coherence = {}\n",
    "\n",
    "for level in df[\"Level\"].unique():\n",
    "    print(f\"\\nLevel: {level}\")\n",
    "\n",
    "    # get index and corpus for each level\n",
    "    level_indices = df.index[df[\"Level\"] == level].tolist()\n",
    "    corpus_level = [corpus[i] for i in level_indices]\n",
    "\n",
    "    tfidf_model = TfidfModel(corpus_level)\n",
    "    corpus_tfidf = tfidf_model[corpus_level]\n",
    "\n",
    "    # TF-IDF matrix construction\n",
    "    num_docs_level = len(corpus_level)\n",
    "    num_terms = len(id2word)\n",
    "    tfidf_matrix = np.zeros((num_docs_level, num_terms))\n",
    "\n",
    "    for doc_id, doc in enumerate(corpus_tfidf):\n",
    "        for term_id, tfidf_score in doc:\n",
    "            tfidf_matrix[doc_id, term_id] = tfidf_score\n",
    "\n",
    "    # topic relevance for each levels document\n",
    "    lda_model_level = level_models[level]\n",
    "    dominant_topics = []\n",
    "    for doc_bow in corpus_level:\n",
    "        topics = lda_model_level.get_document_topics(doc_bow)\n",
    "        dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    "        dominant_topics.append(dominant_topic)\n",
    "\n",
    "    # c-TF-IDF\n",
    "    unique_topics = np.unique(dominant_topics)\n",
    "    num_topics = lda_model_level.num_topics\n",
    "\n",
    "    ctfidf_matrix = np.zeros((num_topics, num_terms))\n",
    "\n",
    "    for topic in unique_topics:\n",
    "        doc_indices = [i for i, t in enumerate(dominant_topics) if t == topic]\n",
    "        ctfidf_matrix[topic] = tfidf_matrix[doc_indices].mean(axis=0)\n",
    "\n",
    "    feature_names = [id2word[i] for i in range(len(id2word))]\n",
    "\n",
    "    coherence_scores = compute_cosine_coherence_ctfidf(ctfidf_matrix, feature_names, top_n=10)\n",
    "\n",
    "    level_coherence[level] = coherence_scores\n",
    "\n",
    "    # results\n",
    "    for topic_id, score in coherence_scores.items():\n",
    "        print(f\"Topic {topic_id}: Cosine Coherence = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egOgwOm70RiI"
   },
   "source": [
    "## Topic Distribution Through Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51581,
     "status": "ok",
     "timestamp": 1750098191124,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "Vk1MtgQc6b1V",
    "outputId": "95fa1eb6-302a-4ec3-882e-3108e0ea0b88"
   },
   "outputs": [],
   "source": [
    "for level, lda_lvl in level_models.items():\n",
    "    # filter docs by level\n",
    "    mask     = df[\"Level\"] == level\n",
    "    df_lvl   = df.loc[mask].reset_index(drop=True)\n",
    "    idxs     = df.index[mask].tolist()\n",
    "    bows_lvl = [corpus[i] for i in idxs]\n",
    "\n",
    "    # infer topic weights\n",
    "    td = [\n",
    "        dict(lda_lvl.get_document_topics(bow, minimum_probability=0))\n",
    "        for bow in bows_lvl\n",
    "    ]\n",
    "    td_df = pd.DataFrame(td)\n",
    "    td_df[\"year_month\"] = df_lvl[\"year_month\"]\n",
    "\n",
    "    # aggregate by month\n",
    "    agg = td_df.groupby(\"year_month\").mean().sort_index()\n",
    "    agg.index = pd.to_datetime(agg.index + \"-01\")\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    for topic_id, series in agg.items():\n",
    "        label = custom_labels[level].get(topic_id, f\"Topic {topic_id}\")\n",
    "        plt.plot(agg.index, series, label=label)\n",
    "\n",
    "    plt.title(f\"Topic Prevalence Over Time โ Level {level}\", fontsize=18, weight=\"bold\")\n",
    "    plt.ylabel(\"Average Topic Weight\", fontsize=14)\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%b %Y\"))\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "    plt.legend(\n",
    "        title=\"Topics\",\n",
    "        bbox_to_anchor=(1.02, 0.5),\n",
    "        loc=\"center left\",\n",
    "        borderaxespad=0,\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "    plt.tight_layout(rect=(0, 0, 0.75, 1))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNtAcmIbjiOQF8vsgMxNq0K",
   "collapsed_sections": [
    "D4FmFHkMzp3l",
    "utIhfKxlzx8i",
    "UcJP3U39Fsim",
    "3hc910WkFzwU",
    "quJqkd4c0JSi",
    "egOgwOm70RiI"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
