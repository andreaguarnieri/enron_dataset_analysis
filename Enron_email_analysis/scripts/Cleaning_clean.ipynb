{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHCFKW5t6boW"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VR0BNtJ0ZLs-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import kagglehub\n",
    "import os\n",
    "from google.colab import drive\n",
    "from google.colab import files\n",
    "import matplotlib.pyplot as plt\n",
    "from email.utils import parsedate_to_datetime\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "import quopri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naWfMo7cZLoA"
   },
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"wcukierski/enron-email-dataset\")\n",
    "df = pd.read_csv(path + \"/emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16493,
     "status": "ok",
     "timestamp": 1750079574307,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "XHZfMpMr3X2o",
    "outputId": "0f459113-d69e-4df1-b946-22dfaf330c11"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYff9QuX3cNy"
   },
   "outputs": [],
   "source": [
    "employees_df = pd.read_csv('/content/drive/MyDrive/NLP_Project/employees_enron.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIJODMAZ6eW2"
   },
   "source": [
    "## Define and Apply Categorization Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1749842083950,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "bOez3CdIZamq",
    "outputId": "591ab978-3822-4f1d-ebe7-963a83787e6c"
   },
   "outputs": [],
   "source": [
    "# Define the categorization rules\n",
    "def categorize_level(row):\n",
    "    job_title = str(row['Job_Title']).strip().lower() if pd.notna(row['Job_Title']) else \"\"\n",
    "    department = str(row['Department']).strip().lower() if pd.notna(row['Department']) else \"\"\n",
    "\n",
    "    # High level positions (exact matches)\n",
    "    high_positions = [\n",
    "        'ceo',\n",
    "        'president & ceo',\n",
    "        'president & coo',\n",
    "        'evp & general counsel',\n",
    "        'evp & cro',\n",
    "        'managing director & cco',\n",
    "        'managing director',\n",
    "        'coo',\n",
    "        'president'  # Standalone president\n",
    "    ]\n",
    "\n",
    "    # Medium level positions\n",
    "    medium_positions = [\n",
    "        'vice president',\n",
    "        'vice president & general counsel',\n",
    "        'vp & assistant general counsel',\n",
    "        'director',\n",
    "        'manager'\n",
    "    ]\n",
    "\n",
    "    # First check for exact matches in high level\n",
    "    if any(job_title == pos.lower() for pos in high_positions):\n",
    "        return 'High'\n",
    "\n",
    "    # Then check for exact matches in medium level\n",
    "    elif any(job_title == pos.lower() for pos in medium_positions):\n",
    "        return 'Medium'\n",
    "\n",
    "    # Special case for Attorneys\n",
    "    elif 'attorney' in job_title:\n",
    "        if 'general counsel' in department or 'assistant general counsel' in department:\n",
    "            return 'Medium'\n",
    "        else:\n",
    "            return 'Low'\n",
    "\n",
    "    # Low level positions (default)\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "# Apply the categorization\n",
    "employees_df['Level'] = employees_df.apply(categorize_level, axis=1)\n",
    "employees_df['First_Name'] = employees_df['First_Name'].str.capitalize()\n",
    "employees_df['Last_Name'] = employees_df['Last_Name'].str.capitalize()\n",
    "print(employees_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jxJ0RpG6m95"
   },
   "source": [
    "## Cleaning and Preparing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4r5gBG956q4r"
   },
   "source": [
    "### Parse Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHU3cyTZZakg"
   },
   "outputs": [],
   "source": [
    "def parse_email(text):\n",
    "    fields = {\n",
    "        'Message-ID': '',\n",
    "        'Date': '',\n",
    "        'From': '',\n",
    "        'To': '',\n",
    "        'Subject': '',\n",
    "        'Body': ''\n",
    "    }\n",
    "\n",
    "    # Extract using regex\n",
    "    headers = {\n",
    "        'Message-ID': re.search(r'Message-ID:\\s*(.+)', text),\n",
    "        'Date': re.search(r'Date:\\s*(.+)', text),\n",
    "        'From': re.search(r'From:\\s*(.+)', text),\n",
    "        'To': re.search(r'To:\\s*(.+)', text),\n",
    "        'Subject': re.search(r'Subject:\\s*(.+)', text),\n",
    "    }\n",
    "\n",
    "    for key, match in headers.items():\n",
    "        if match:\n",
    "            fields[key] = match.group(1).strip()\n",
    "\n",
    "    body_split = re.split(r'\\n\\s*\\n', text, maxsplit=1)\n",
    "    if len(body_split) > 1:\n",
    "        fields['Body'] = body_split[1].strip()\n",
    "\n",
    "    return pd.Series(fields)\n",
    "\n",
    "df_parsed = df['message'].apply(parse_email)\n",
    "df_final = pd.concat([df, df_parsed], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1749842191475,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "8Cxl_9N04F1K",
    "outputId": "448cf9c8-69db-4c8c-da2c-cc427cd0af84"
   },
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BspD3Ged6tUa"
   },
   "source": [
    "### Filter by Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63647,
     "status": "ok",
     "timestamp": 1749842255124,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "xip0swAgsIFy",
    "outputId": "6b2e234f-5981-48b7-ffa7-256eb0e74642"
   },
   "outputs": [],
   "source": [
    "df_final['Date'] = pd.to_datetime(df_final['Date'], errors='coerce', utc = True)\n",
    "print(df_final['Date'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgNC76wdsQYt"
   },
   "outputs": [],
   "source": [
    "start_date = '1998-01-01'\n",
    "end_date   = '2004-12-31'\n",
    "\n",
    "mask = (df_final['Date'] >= start_date) & (df_final['Date'] <= end_date)\n",
    "df_final = df_final.loc[mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyugCKnL6x0K"
   },
   "source": [
    "### Filter Allowed Email Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1Ci1lve2GO4"
   },
   "outputs": [],
   "source": [
    "allowed_domains = ['@enron', '@yahoo', '@hotmail', '@outlook']\n",
    "\n",
    "#keeping only rows with valid domains\n",
    "df_final = df_final[df_final['From'].str.contains('|'.join(allowed_domains), na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rB24p6cm61Pb"
   },
   "source": [
    "### Extract Sender/Recipient Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgNvvTWfZafN"
   },
   "outputs": [],
   "source": [
    "# Extract name and surname from the email checking for all the possible patterns\n",
    "def extract_name_surname(email, known_names, known_surnames):\n",
    "    email_prefix = email.split('@')[0].lower()\n",
    "\n",
    "    def clean(part):\n",
    "        return re.sub(r'\\d+', '', part)\n",
    "\n",
    "    # Split by dot or underscore ignoring empty parts\n",
    "    if '.' in email_prefix:\n",
    "        parts = [clean(p) for p in email_prefix.split('.') if p.strip()]\n",
    "        if len(parts) == 2:\n",
    "            part1, part2 = parts\n",
    "\n",
    "            # Handle initial case: if part1 is single char initial\n",
    "            if len(part1) == 1:\n",
    "                # Try to find full first name by surname (part2)\n",
    "                matches = employees_df[employees_df['Last_Name'].str.lower() == part2]\n",
    "                if len(matches) == 1:\n",
    "                    return matches['First_Name'].values[0].capitalize(), part2.capitalize()\n",
    "                else:\n",
    "                    return part1.upper(), part2.capitalize()\n",
    "\n",
    "            part1_is_name = part1.capitalize() in known_names.values\n",
    "            part2_is_surname = part2.capitalize() in known_surnames.values\n",
    "            part2_is_name = part2.capitalize() in known_names.values\n",
    "            part1_is_surname = part1.capitalize() in known_surnames.values\n",
    "\n",
    "            if part1_is_name and part2_is_surname:\n",
    "                return part1.capitalize(), part2.capitalize()\n",
    "            elif part2_is_name and part1_is_surname:\n",
    "                return part2.capitalize(), part1.capitalize()\n",
    "            else:\n",
    "                return part1.capitalize(), part2.capitalize()\n",
    "        elif len(parts) == 3:\n",
    "            first, last = clean(parts[0]), clean(parts[2])\n",
    "            if len(first) == 1:\n",
    "                # Try to find full first name by surname (part2)\n",
    "                matches = employees_df[employees_df['Last_Name'].str.lower() == last]\n",
    "                if len(matches) == 1:\n",
    "                    return matches['First_Name'].values[0].capitalize(), last.capitalize()\n",
    "                else:\n",
    "                    return first.upper(), last.capitalize()\n",
    "\n",
    "            part1_is_name = first.capitalize() in known_names.values\n",
    "            part2_is_surname = last.capitalize() in known_surnames.values\n",
    "            part2_is_name = last.capitalize() in known_names.values\n",
    "            part1_is_surname = first.capitalize() in known_surnames.values\n",
    "\n",
    "            if part1_is_name and part2_is_surname:\n",
    "                return first.capitalize(), last.capitalize()\n",
    "            elif part2_is_name and part1_is_surname:\n",
    "                return last.capitalize(), first.capitalize()\n",
    "            else:\n",
    "                return first.capitalize(), last.capitalize()\n",
    "\n",
    "    elif '_' in email_prefix:\n",
    "        parts = [clean(p) for p in email_prefix.split('_') if p.strip()]\n",
    "        if len(parts) == 2:\n",
    "            part1, part2 = parts\n",
    "\n",
    "            if len(part1) == 1:\n",
    "                matches = employees_df[employees_df['Last_Name'].str.lower() == part2]\n",
    "                if len(matches) == 1:\n",
    "                    return matches['First_Name'].values[0].capitalize(), part2.capitalize()\n",
    "                else:\n",
    "                    return part1.upper(), part2.capitalize()\n",
    "\n",
    "            part1_is_name = part1.capitalize() in known_names.values\n",
    "            part2_is_surname = part2.capitalize() in known_surnames.values\n",
    "            part2_is_name = part2.capitalize() in known_names.values\n",
    "            part1_is_surname = part1.capitalize() in known_surnames.values\n",
    "\n",
    "            if part1_is_name and part2_is_surname:\n",
    "                return part1.capitalize(), part2.capitalize()\n",
    "            elif part2_is_name and part1_is_surname:\n",
    "                return part2.capitalize(), part1.capitalize()\n",
    "            else:\n",
    "                return part1.capitalize(), part2.capitalize()\n",
    "\n",
    "    pattern = re.compile(r'([a-z]+)\\d*([a-z]+)')\n",
    "    match = pattern.match(email_prefix)\n",
    "    if match:\n",
    "        first, last = match.groups()\n",
    "        if first in known_names.values and last in known_surnames.values:\n",
    "            return first.capitalize(), last.capitalize()\n",
    "\n",
    "    found_name = next((name for name in known_names if name.lower() in email_prefix), None)\n",
    "    found_surname = next((surname for surname in known_surnames if surname.lower() in email_prefix), None)\n",
    "\n",
    "    if found_name and not found_surname:\n",
    "        matches = employees_df[employees_df['First_Name'].str.lower() == found_name.lower()]\n",
    "        if len(matches) == 1:\n",
    "            return found_name.capitalize(), matches['Last_Name'].values[0].capitalize()\n",
    "        else:\n",
    "            return found_name.capitalize(), 'Unknown'\n",
    "\n",
    "    if found_surname and not found_name:\n",
    "        matches = employees_df[employees_df['Last_Name'].str.lower() == found_surname.lower()]\n",
    "        if len(matches) == 1:\n",
    "            return matches['First_Name'].values[0].capitalize(), found_surname.capitalize()\n",
    "        else:\n",
    "            return 'Unknown', found_surname.capitalize()\n",
    "\n",
    "    if found_name and found_surname:\n",
    "        return found_name.capitalize(), found_surname.capitalize()\n",
    "\n",
    "    return 'Unknown', 'Unknown'\n",
    "\n",
    "known_names = employees_df['First_Name']\n",
    "known_surnames = employees_df['Last_Name']\n",
    "first_names, last_names, mails = [], [], []\n",
    "for mail in df_final['From']:\n",
    "    first, last = extract_name_surname(mail, known_names, known_surnames)\n",
    "    first_names.append(first)\n",
    "    last_names.append(last)\n",
    "    if pd.notnull(mail) and '@' in mail:\n",
    "        mails.append(mail.split('@')[1])\n",
    "    else:\n",
    "        mails.append('Unknown')\n",
    "df_final['From_First_Name'] = first_names\n",
    "df_final['From_Last_Name'] = last_names\n",
    "df_final['From_Mail'] = mails\n",
    "df_final['From_External'] = df_final['From_Mail'].apply(lambda x: 'enron.com' not in x)\n",
    "\n",
    "first_names, last_names, mails = [], [], []\n",
    "for mail in df_final['To']:\n",
    "    first, last = extract_name_surname(mail, known_names, known_surnames)\n",
    "    first_names.append(first)\n",
    "    last_names.append(last)\n",
    "    if pd.notnull(mail) and '@' in mail:\n",
    "        mails.append(mail.split('@')[1])\n",
    "    else:\n",
    "        mails.append('Unknown')\n",
    "df_final['To_First_Name'] = first_names\n",
    "df_final['To_Last_Name'] = last_names\n",
    "df_final['To_Mail'] = mails\n",
    "df_final['To_External'] = df_final['To_Mail'].apply(lambda x: 'enron.com' not in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "executionInfo": {
     "elapsed": 1046,
     "status": "ok",
     "timestamp": 1749842319592,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "i7cDs4VCZlmf",
    "outputId": "50309325-0b0a-4fe6-9c03-ed158e0ba0df"
   },
   "outputs": [],
   "source": [
    "# Merge with employee data\n",
    "df_final = df_final.merge(employees_df,\n",
    "                         left_on=['From_First_Name', 'From_Last_Name'],\n",
    "                         right_on=['First_Name', 'Last_Name'],\n",
    "                         how='left')\n",
    "df_final.drop(columns=['First_Name', 'Last_Name', 'Job_Title', 'Department'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 448,
     "status": "ok",
     "timestamp": 1749842320042,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "gN6otKPizI7V",
    "outputId": "beadc60b-0588-4133-9bfd-69c564dc2bba"
   },
   "outputs": [],
   "source": [
    "df_final.loc[(df_final['Level'].isna()) & (df_final['From'].str.contains('@enron.com'))].groupby('From').agg(n_mail=('From', 'count')).sort_values('n_mail', ascending=False).head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1749842320324,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "zlS2wNAZZubJ",
    "outputId": "a8f5832e-7bed-4e32-fde5-be8ecee97cfa"
   },
   "outputs": [],
   "source": [
    "#visualization of the category division\n",
    "counts = employees_df[\"Level\"].value_counts().reindex([\"High\", \"Medium\", \"Low\"])\n",
    "\n",
    "#bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = counts.plot(kind=\"bar\", color=[\"#4C78A8\", \"#54A24B\", \"#E45756\", \"#79706E\"], edgecolor=\"black\")\n",
    "\n",
    "plt.title(\"Enron Employees by Job Category Level\", fontsize=14, pad=20)\n",
    "plt.xlabel(\"Level\", fontsize=12)\n",
    "plt.ylabel(\"Number of Employees\", fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "for bar in bars.patches:\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 1,\n",
    "        f\"{int(bar.get_height())}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=10\n",
    "    )\n",
    "plt.tight_layout()\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1749842320361,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "XTuwKCNszyLU",
    "outputId": "eb5504aa-30de-4cfa-d763-99b64375f344"
   },
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arN0f5JG8TdK"
   },
   "source": [
    "## Cleaning Email Body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vF2UpZJd7Rw-"
   },
   "source": [
    "### Non-Text and Paragraph Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uzXCVD2aoGys"
   },
   "outputs": [],
   "source": [
    "def remove_signature_block(text, min_signals=3):\n",
    "    signature_keywords = [\n",
    "        #r'@',\n",
    "        r'fax\\b', r'phone\\b', r'mobile\\b', r'tel\\b',\n",
    "        r'street\\b', r'avenue\\b', r'floor\\b', r'suite\\b',\n",
    "        r'houston', r'tx\\b', r'enron\\b', r'corp\\b', r'company\\b',\n",
    "        r'(director|manager|assistant|specialist|executive|coordinator)',\n",
    "        r'\\d{3}[-/\\.\\s]?\\d{3}[-/\\.\\s]?\\d{4}',\n",
    "        r'\\(\\d{3}\\)\\s*\\d{3}-\\d{4}',\n",
    "        r'https?://', r'www\\.',\n",
    "        r'\\.com\\b', r'\\.org\\b', r'\\.edu\\b'\n",
    "    ]\n",
    "\n",
    "    lines = text.strip().split('\\n')\n",
    "    # Start from the bottom and collect a block with no blank lines\n",
    "    block = []\n",
    "    for line in reversed(lines):\n",
    "        if not line.strip():  # Empty line -> stop block\n",
    "            break\n",
    "        block.insert(0, line)\n",
    "\n",
    "    # Evaluate the block\n",
    "    signal_count = sum(\n",
    "        any(re.search(pat, line, re.IGNORECASE) for pat in signature_keywords)\n",
    "        for line in block\n",
    "        if line.strip()\n",
    "    )\n",
    "\n",
    "    if signal_count >= min_signals:\n",
    "        return \"\\n\".join(lines[:-len(block)]).strip()\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bkr2fOqNYZx7"
   },
   "outputs": [],
   "source": [
    "def remove_reply_block(text):\n",
    "    lines = text.splitlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if re.match(r'^\\s*(to|cc|bcc|subject):', line, re.IGNORECASE):\n",
    "            return \"\\n\".join(lines[:i]).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjkTIJTH5raE"
   },
   "outputs": [],
   "source": [
    "def decode_quoted_printable(text):\n",
    "    return quopri.decodestring(text).decode('utf-8', errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kn7pGHcfH80B"
   },
   "outputs": [],
   "source": [
    "def remove_short_closing_signature(text):\n",
    "    lines = text.strip().split('\\n')\n",
    "    if not lines:\n",
    "        return text\n",
    "\n",
    "    # checks last 2 non empy lines\n",
    "    non_empty_lines = [line.strip() for line in lines if line.strip()]\n",
    "    if len(non_empty_lines) == 0:\n",
    "        return text\n",
    "\n",
    "    last_line = non_empty_lines[-1]\n",
    "\n",
    "    # Signature on separate line after . , or ?\n",
    "    if len(non_empty_lines) >= 2:\n",
    "        second_last_line = non_empty_lines[-2]\n",
    "        if re.fullmatch(r'([A-Z][a-z]+|[A-Z]{1,2})( [A-Z][a-z]+)?', last_line.strip()) and re.search(r'[.,?!]\\s*$', second_last_line):\n",
    "            return '\\n'.join(lines[:-1]).strip()\n",
    "\n",
    "    # Signature on same line after . , or ?\n",
    "    if re.search(r'[.,?!]\\s+([A-Z][a-z]+(?: [A-Z][a-z]+)?|[A-Z]{1,2})$', last_line):\n",
    "        match = re.search(r'^(.*?[.,?!])\\s+([A-Z][a-z]+(?: [A-Z][a-z]+)?|[A-Z]{1,2})$', last_line)\n",
    "        if match:\n",
    "            new_last_line = match.group(1)\n",
    "            for i in range(len(lines) - 1, -1, -1):\n",
    "                if lines[i].strip() == last_line:\n",
    "                    return '\\n'.join(lines[:i] + [new_last_line]).strip()\n",
    "\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKk3i_rL1o90"
   },
   "outputs": [],
   "source": [
    "def remove_confidentiality_disclaimer(text):\n",
    "    disclaimer_keywords = [\n",
    "        \"privileged\", \"confidential\", \"confidentiality notice\", \"not the intended recipient\",\n",
    "        \"received in error\", \"please delete\", \"dissemination\", \"unauthorized\",\n",
    "        \"review\", \"joint defense agreement\", \"legal disclaimer\", \"thank you\"\n",
    "    ]\n",
    "\n",
    "    # Split into lines and search near the end\n",
    "    lines = text.strip().splitlines()\n",
    "    start = max(0, len(lines) - 15)  # look only at last 15 lines\n",
    "    tail = lines[start:]\n",
    "\n",
    "    block = \"\\n\".join(tail).lower()\n",
    "    keyword_hits = sum(1 for kw in disclaimer_keywords if kw in block)\n",
    "\n",
    "    if keyword_hits >= 3 and len(block) > 300:\n",
    "        # Remove all text from the first disclaimer keyword hit to end\n",
    "        for i, line in enumerate(tail):\n",
    "            if any(kw in line.lower() for kw in disclaimer_keywords):\n",
    "                return \"\\n\".join(lines[:start + i]).strip()\n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Zy8aciWJhpL"
   },
   "outputs": [],
   "source": [
    "def clean_email_body(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Removes lines that start with >\n",
    "    text = '\\n'.join(line for line in text.splitlines() if not line.strip().startswith('>'))\n",
    "\n",
    "    # Removes lines like [date][name] wrote:\"\n",
    "    text = re.sub(r'^On \\d{1,2}/\\d{1,2}/\\d{2,4} .{1,50}? wrote:\\s*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Removes everything from \"Forwarded by\" or \"Original Message\"\n",
    "    text = re.split(r'(?im)^[-\\s]*\\b(forwarded by|original message)\\b.*', text, maxsplit=1)[0]\n",
    "\n",
    "    # removes reply blocks (To:, Cc:, Subject: ecc.)\n",
    "    text = remove_reply_block(text)\n",
    "\n",
    "    # Removes signature blocks\n",
    "    text = remove_signature_block(text)\n",
    "\n",
    "    # Removes attachments\n",
    "    text = re.sub(r'\\b[\\w\\-]+\\.(pdf|docx?|xlsx?|csv|json|pptx?|txt|zip|jpg|pcx)\\b', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Decode content encoded in quoted-printable\n",
    "    text = decode_quoted_printable(text)\n",
    "\n",
    "    # Removes links\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Removes disclaimers\n",
    "    text = remove_confidentiality_disclaimer(text) #check\n",
    "\n",
    "    # Removs timestamps\n",
    "    text = re.sub(r'^\\s*\\d{2}/\\d{2}/\\d{4} \\d{2}:\\d{2} [AP]M\\s*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Removes emails within <>\n",
    "    text = re.sub(r'^.*<[^@<>\\s]+@[^<>\\s]+>.*$\\n?', '', text, flags=re.MULTILINE) #check\n",
    "\n",
    "    # Removes lines with emails @ENRON o @ECT\n",
    "    text = re.sub(r'^[ \\t]*[A-Za-z]+(?: [A-Za-z]+)*@(ENRON|ECT)[ \\t]*\\n?', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Removes IMAGE\n",
    "    text = re.sub(r'\\bIMAGE\\b', '', text)\n",
    "\n",
    "    # Removes all non-text characters except punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?;:'\\\"()\\n ]+\", \"\", text)\n",
    "\n",
    "    # Removes empty parenthesis or paranthesis that contain something other than letters\n",
    "    text = re.sub(r'\\(\\s*[^\\w\\s]*\\s*\\)', '', text)\n",
    "\n",
    "    # Removes lines starting with \"From:\" or \"Sent by:\"\n",
    "    text = re.sub(r'^\\s*(From|Sent by):\\s+.*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Removes short signatures\n",
    "    text = remove_short_closing_signature(text)\n",
    "\n",
    "    # Removes multiple empty lines\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "\n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mG8gVA-gTm3E"
   },
   "outputs": [],
   "source": [
    "df_filtered = df_final.loc[df_final['Level'].notna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAAhfXteMBRX"
   },
   "outputs": [],
   "source": [
    "df_filtered['Cleaned_Body'] = df_filtered['Body'].apply(clean_email_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5doaNX1-xkM"
   },
   "source": [
    "### Sentence Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ma2iDkIKltfl"
   },
   "outputs": [],
   "source": [
    "# Sentence Normalization\n",
    "\n",
    "def sentence_normalization(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Handle \\n and cleanup\n",
    "    text_repr = repr(text).strip(\"'\\\"\")\n",
    "\n",
    "    # Special rule: replace comma with dot if first line has one word ending with comma\n",
    "    first_line = text_repr.split('\\\\n')[0]\n",
    "    if re.match(r'^[A-Za-z]+[,]$', first_line.strip()):\n",
    "        text_repr = text_repr.replace(first_line, first_line.replace(',', '.'), 1)\n",
    "\n",
    "    # Replace any word ending in : with a .\n",
    "    text_repr = re.sub(r'(\\w+):', r'\\1.', text_repr)\n",
    "\n",
    "    # Remove space(s) before a period (e.g., \"word  .\" → \"word.\")\n",
    "    text_repr = re.sub(r'\\s+\\.', '.', text_repr)\n",
    "\n",
    "    # Add punctuation before \\n if missing\n",
    "    text_repr = re.sub(r'(?<![.!?,])\\\\n(?=[A-Z])', r'.\\\\n', text_repr)\n",
    "\n",
    "    # Flatten and clean lines\n",
    "    parts = text_repr.split('\\\\n')\n",
    "    text = ' '.join(p.strip() for p in parts if p.strip())\n",
    "\n",
    "    # Normalize repeated punctuation\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    text = re.sub(r'!{2,}', '!', text)\n",
    "    text = re.sub(r'\\?{2,}', '?', text)\n",
    "\n",
    "    # Add space after punctuation if missing\n",
    "    text = re.sub(r'([.!?])([^\\s\"\\'])', r'\\1 \\2', text)\n",
    "\n",
    "    # Sentence splitting and filtering\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip()) > 2 and any(c.isalpha() for c in s)]\n",
    "\n",
    "    return ' '.join(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EduHLBcfl8qS"
   },
   "outputs": [],
   "source": [
    "df_filtered['Cleaned_Body_n'] = df_filtered['Cleaned_Body'].apply(sentence_normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLeZhuf3-zvh"
   },
   "source": [
    "### Word Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PwFVxV8k1-c"
   },
   "outputs": [],
   "source": [
    "# Word normalization\n",
    "\n",
    "acronyms = {\"CEO\", \"CFO\", \"IBM\", \"SEC\", \"ENRON\", \"FBI\", \"IRS\", \"USA\", \"Nokia\"}\n",
    "\n",
    "def restore_case_minimal(text, acronyms=acronyms):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    words = text.split()\n",
    "    result = []\n",
    "    capitalize_next = True\n",
    "\n",
    "    for word in words:\n",
    "        base = word.strip('.,!?')  # remove trailing punctuation\n",
    "\n",
    "        if not base:\n",
    "            result.append(word)\n",
    "            capitalize_next = word.endswith('.') or word.endswith('!') or word.endswith('?')\n",
    "            continue\n",
    "\n",
    "        # Handle acronyms and capitalization after punctuation\n",
    "        if base.upper() in acronyms:\n",
    "            fixed = base.upper()\n",
    "        elif capitalize_next and not base[0].isupper():\n",
    "            fixed = base.capitalize()\n",
    "        else:\n",
    "            fixed = base\n",
    "\n",
    "        suffix = word[len(base):]  # restore punctuation\n",
    "        result.append(fixed + suffix)\n",
    "\n",
    "        capitalize_next = word.endswith('.') or word.endswith('!') or word.endswith('?')\n",
    "\n",
    "    return ' '.join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TDwUkYhtm2vc"
   },
   "outputs": [],
   "source": [
    "df_filtered['Cleaned_Body_n'] = df_filtered['Cleaned_Body_n'].apply(restore_case_minimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UI_1TpY0-e1X"
   },
   "outputs": [],
   "source": [
    "emails_to_remove = [\n",
    "    'ibuyit@enron.com', 'getmorechannels@hotmail.com', 'cluwhite@hotmail.com',\n",
    "    'enron.general.announcements.enronxgate@enron.com',\n",
    "    'gary.allen.-.safety.specialist@enron.com', 'budanski@hotmail.com',\n",
    "    'maggiemschf@yahoo.com', 'getmorechannels2@hotmail.com',\n",
    "    'alerts-breakingnews@yahoo-inc.com', 'malanga45@yahoo.com',\n",
    "    'alltimeqb@hotmail.com', 'lizard6849@yahoo.com', 'buy@enron.com',\n",
    "    'black@enron.com', 'zipper@enron.com', 'neale@enron.com'\n",
    "]\n",
    "\n",
    "# Rimuovi le righe in cui il campo 'From' è in emails_to_remove\n",
    "df_filtered = df_filtered[~df_filtered['From'].isin(emails_to_remove)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1749842440802,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "GvQ0dnFHScqd",
    "outputId": "dc02c656-27bc-4094-b45a-cadff1a6856a"
   },
   "outputs": [],
   "source": [
    "len(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1749842440879,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "jqEt4Wey_fYJ",
    "outputId": "7927902c-adf4-43bc-d45e-4b5e8d1e10d0"
   },
   "outputs": [],
   "source": [
    "df_filtered['From'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cqZcnjO-2Xe"
   },
   "source": [
    "## Saving the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yV8CWGDTk6Od"
   },
   "outputs": [],
   "source": [
    "df_filtered.to_csv('/content/drive/MyDrive/NLP_Project/df_clean.csv', sep='§')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "hHCFKW5t6boW",
    "mIJODMAZ6eW2",
    "_jxJ0RpG6m95",
    "4r5gBG956q4r",
    "BspD3Ged6tUa",
    "VyugCKnL6x0K",
    "rB24p6cm61Pb",
    "arN0f5JG8TdK",
    "vF2UpZJd7Rw-",
    "R5doaNX1-xkM",
    "cLeZhuf3-zvh",
    "1cqZcnjO-2Xe"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
