{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YU23NYF9x4Il"
   },
   "source": [
    "## Importing Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1750107955875,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "P8s3xYpnxlq7",
    "outputId": "81376034-89ca-457f-b100-72532e841019"
   },
   "outputs": [],
   "source": [
    "import csv, sys\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66974,
     "status": "ok",
     "timestamp": 1750108025453,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "2CnU2v7Nx7E9",
    "outputId": "1052168b-1683-4b1c-d2d5-30d33cef370d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/data.csv',sep='§')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1819,
     "status": "ok",
     "timestamp": 1750108027263,
     "user": {
      "displayName": "Giada Poloni",
      "userId": "05088726081194754954"
     },
     "user_tz": -120
    },
    "id": "TNJFE5GjKuDQ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWfRloJZNy99"
   },
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eFiM0EnOczT"
   },
   "source": [
    "### Implementing XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2SykjIr4OeQ"
   },
   "outputs": [],
   "source": [
    "# classification with XGBoost\n",
    "df = df[df['Level'].isin(['High', 'Medium', 'Low'])]\n",
    "df['Level'] = df['Level'].astype(str)\n",
    "\n",
    "# 1 new features\n",
    "# 1.1 emails from 20 pm to 6 am\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "df['hour'] = df['Date'].dt.hour\n",
    "df['is_night'] = df['hour'].apply(lambda x: 1 if x >= 20 or x < 6 else 0)\n",
    "\n",
    "# 1.2 num_recipients\n",
    "df['num_recipients'] = df['To'].fillna('').apply(lambda x: len(x.split(',')) if x else 0)\n",
    "\n",
    "# 1.3 Emails sent to executives (To_External == False & Level == ‘High’)\n",
    "df['to_executive'] = ((df['To_External'] == False) & (df['Level'] == 'High')).astype(int)\n",
    "\n",
    "# 1.4 dummy for verbosity\n",
    "df['verbosity'] = df['verbosity'].astype(str)  # assicurati che sia stringa\n",
    "verbosity_dummies = pd.get_dummies(df['verbosity'], prefix='verbosity')\n",
    "df = pd.concat([df, verbosity_dummies], axis=1)\n",
    "df = df.drop(columns=['verbosity'])\n",
    "\n",
    "numeric_cols = [\n",
    "    'char_length', 'word_length', 'avg_word_length',\n",
    "    'mtld', 'mattr',\n",
    "    'is_night', 'num_recipients', 'to_executive'\n",
    "] + verbosity_dummies.columns.tolist()\n",
    "\n",
    "\n",
    "def add_keyword_features(df):\n",
    "    Courtesy = ['please', 'thanks', 'thank', 'regards', 'sincerely', 'appreciate', 'best']\n",
    "    Orders = ['confirm', 'forward', 'schedule', 'prepare', 'submit', 'review', 'organize', 'must', 'fix', 'should']\n",
    "    Crisis = ['crisis', 'issue', 'problem', 'concern', 'urgent', 'delay', 'fail', 'risk', 'pressure']\n",
    "\n",
    "    # keyword group\n",
    "    def keyword_flag(df, keyword_list, group_name):\n",
    "        pattern = r'\\b(?:' + '|'.join(map(re.escape, keyword_list)) + r')\\b'\n",
    "        col_name = f'has_{group_name}'\n",
    "        df[col_name] = df['Cleaned_Body_n'].str.contains(pattern, case=False, na=False).astype(int)\n",
    "        return col_name\n",
    "\n",
    "    new_cols = []\n",
    "    new_cols.append(keyword_flag(df, Courtesy, 'courtesy'))\n",
    "    new_cols.append(keyword_flag(df, Orders, 'orders'))\n",
    "    new_cols.append(keyword_flag(df, Crisis, 'crisis'))\n",
    "\n",
    "    return new_cols\n",
    "\n",
    "keyword_feature_cols = add_keyword_features(df)\n",
    "numeric_cols += keyword_feature_cols\n",
    "\n",
    "# to numeric (in order to avoid errors)\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# 2 Stratified Split: 2/3 train, 1/3 test\n",
    "train_df, test_df = train_test_split(df, test_size=1/3, random_state=42, stratify=df['Level'])\n",
    "\n",
    "X_train_text = train_df['Cleaned_Body_n'].fillna('')\n",
    "X_test_text = test_df['Cleaned_Body_n'].fillna('')\n",
    "y_train = train_df['Level']\n",
    "y_test = test_df['Level']\n",
    "\n",
    "X_train_numeric = train_df[numeric_cols]\n",
    "X_test_numeric = test_df[numeric_cols]\n",
    "\n",
    "# 3 preprocessing: TF-IDF + scaling\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "# 4 pipeline\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# 5 combine with ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"tfidf\", tfidf, \"text\"),\n",
    "    (\"num\", numeric_transformer, numeric_cols)\n",
    "])\n",
    "\n",
    "X_train_all = train_df[numeric_cols].copy()\n",
    "X_test_all = test_df[numeric_cols].copy()\n",
    "X_train_all['text'] = X_train_text\n",
    "X_test_all['text'] = X_test_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 120322,
     "status": "ok",
     "timestamp": 1750080217703,
     "user": {
      "displayName": "Andrea Guarnieri",
      "userId": "03250951868602502178"
     },
     "user_tz": -120
    },
    "id": "sW03EO0A5g4N",
    "outputId": "3f67af2a-1ff8-42ec-86ac-6db5289f23a8"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# 6 XGBoost\n",
    "model = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"clf\", XGBClassifier(\n",
    "        objective='multi:softmax',\n",
    "        num_class=3,\n",
    "        eval_metric='mlogloss',\n",
    "        use_label_encoder=False,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "# 7 compute sample weights\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train_enc)\n",
    "\n",
    "# 8 fit\n",
    "model.fit(X_train_all, y_train_enc, clf__sample_weight=sample_weights)\n",
    "y_pred_enc = model.predict(X_test_all)\n",
    "\n",
    "y_pred = le.inverse_transform(y_pred_enc)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 874,
     "status": "ok",
     "timestamp": 1750080218573,
     "user": {
      "displayName": "Andrea Guarnieri",
      "userId": "03250951868602502178"
     },
     "user_tz": -120
    },
    "id": "wkWvi0XBTsBz",
    "outputId": "40f3fab7-4594-41fe-8e7d-ba78e30717a4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Confusion matrix\n",
    "cm_email = confusion_matrix(y_test, y_pred, labels=le.classes_)\n",
    "cm_email_norm = cm_email.astype('float') / cm_email.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    cm_email_norm,\n",
    "    annot=True,\n",
    "    fmt='.2%',\n",
    "    cmap='Blues',\n",
    "    xticklabels=le.classes_,\n",
    "    yticklabels=le.classes_\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix at email level (percentages per class)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lS9ocb5_Jux"
   },
   "source": [
    "### Feature importance for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1750080218594,
     "user": {
      "displayName": "Andrea Guarnieri",
      "userId": "03250951868602502178"
     },
     "user_tz": -120
    },
    "id": "IM4A-v2m2RC1",
    "outputId": "a5047f56-2b27-4176-a39b-3009717201c3"
   },
   "outputs": [],
   "source": [
    "# XGBoost booster\n",
    "clf = model.named_steps[\"clf\"]\n",
    "booster = clf.get_booster()\n",
    "importance_dict = booster.get_score(importance_type='gain')\n",
    "\n",
    "numeric_features = numeric_cols\n",
    "tfidf_features = list(preprocessor.named_transformers_['tfidf'].get_feature_names_out())\n",
    "all_features = numeric_features + tfidf_features\n",
    "\n",
    "mapped_importance = {\n",
    "    all_features[int(f[1:])]: v for f, v in importance_dict.items()\n",
    "    if int(f[1:]) < len(all_features)\n",
    "}\n",
    "\n",
    "# TF-IDF vs. others\n",
    "final_importance = {}\n",
    "tfidf_total = 0\n",
    "\n",
    "for feat, score in mapped_importance.items():\n",
    "    if feat in tfidf_features:\n",
    "        tfidf_total += score\n",
    "    else:\n",
    "        final_importance[feat] = score\n",
    "\n",
    "# aggregate TF-IDF\n",
    "final_importance['tfidf'] = tfidf_total\n",
    "\n",
    "# sort by importance\n",
    "sorted_importance = sorted(final_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "for feat, score in sorted_importance:\n",
    "    print(f\"{feat:30} {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1750080218822,
     "user": {
      "displayName": "Andrea Guarnieri",
      "userId": "03250951868602502178"
     },
     "user_tz": -120
    },
    "id": "wJxhbwj73rcU",
    "outputId": "b6d931b1-9d3f-49e5-86f5-d8d3c4bac249"
   },
   "outputs": [],
   "source": [
    "# filter features, excluding 'tfidf'\n",
    "filtered_importance = {feat: score for feat, score in final_importance.items() if feat != 'tfidf'}\n",
    "\n",
    "# sort\n",
    "sorted_items = sorted(filtered_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "features, scores = zip(*sorted_items)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(features, scores, color='skyblue')\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIXYmLxTx0UC"
   },
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1750080230954,
     "user": {
      "displayName": "Andrea Guarnieri",
      "userId": "03250951868602502178"
     },
     "user_tz": -120
    },
    "id": "fhcRBNY_xo7s",
    "outputId": "b9ca5d46-8c8f-465c-bf33-4a1c146ee381"
   },
   "outputs": [],
   "source": [
    "import csv, sys\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40973,
     "status": "ok",
     "timestamp": 1750080273295,
     "user": {
      "displayName": "Andrea Guarnieri",
      "userId": "03250951868602502178"
     },
     "user_tz": -120
    },
    "id": "6c5ysSc-xp9j",
    "outputId": "733a6c64-c777-4e41-c569-dfc696179a21"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "df = pd.read_csv(\n",
    "    '/content/drive/MyDrive/NLP_Project/data.csv',\n",
    "    sep='§')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpDjn8X2xsdJ"
   },
   "outputs": [],
   "source": [
    "# classification with Support Vector Machine\n",
    "df = df[df['Level'].isin(['High', 'Medium', 'Low'])]\n",
    "df['Level'] = df['Level'].astype(str)\n",
    "\n",
    "# 1 new features\n",
    "# 1.1 emails from 20 pm to 6 am\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "df['hour'] = df['Date'].dt.hour\n",
    "df['is_night'] = df['hour'].apply(lambda x: 1 if x >= 20 or x < 6 else 0)\n",
    "\n",
    "# 1.2 num_recipients\n",
    "df['num_recipients'] = df['To'].fillna('').apply(lambda x: len(x.split(',')) if x else 0)\n",
    "\n",
    "# 1.3 Emails sent to executives (To_External == False & Level == ‘High’)\n",
    "df['to_executive'] = ((df['To_External'] == False) & (df['Level'] == 'High')).astype(int)\n",
    "\n",
    "# 1.4 dummy for verbosity\n",
    "df['verbosity'] = df['verbosity'].astype(str)  # assicurati che sia stringa\n",
    "verbosity_dummies = pd.get_dummies(df['verbosity'], prefix='verbosity')\n",
    "df = pd.concat([df, verbosity_dummies], axis=1)\n",
    "df = df.drop(columns=['verbosity'])\n",
    "\n",
    "numeric_cols = [\n",
    "    'char_length', 'word_length', 'avg_word_length',\n",
    "    'mtld', 'mattr',\n",
    "    'is_night', 'num_recipients', 'to_executive'\n",
    "] + verbosity_dummies.columns.tolist()\n",
    "\n",
    "\n",
    "def add_keyword_features(df):\n",
    "    Courtesy = ['please', 'thanks', 'thank', 'regards', 'sincerely', 'appreciate', 'best']\n",
    "    Orders = ['confirm', 'forward', 'schedule', 'prepare', 'submit', 'review', 'organize', 'must', 'fix', 'should']\n",
    "    Crisis = ['crisis', 'issue', 'problem', 'concern', 'urgent', 'delay', 'fail', 'risk', 'pressure']\n",
    "\n",
    "    # keyword group\n",
    "    def keyword_flag(df, keyword_list, group_name):\n",
    "        pattern = r'\\b(?:' + '|'.join(map(re.escape, keyword_list)) + r')\\b'\n",
    "        col_name = f'has_{group_name}'\n",
    "        df[col_name] = df['Cleaned_Body_n'].str.contains(pattern, case=False, na=False).astype(int)\n",
    "        return col_name\n",
    "\n",
    "    new_cols = []\n",
    "    new_cols.append(keyword_flag(df, Courtesy, 'courtesy'))\n",
    "    new_cols.append(keyword_flag(df, Orders, 'orders'))\n",
    "    new_cols.append(keyword_flag(df, Crisis, 'crisis'))\n",
    "\n",
    "    return new_cols\n",
    "\n",
    "keyword_feature_cols = add_keyword_features(df)\n",
    "numeric_cols += keyword_feature_cols\n",
    "\n",
    "# to numeric (in order to avoid errors)\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# 2 Stratified Split: 2/3 train, 1/3 test\n",
    "train_df, test_df = train_test_split(df, test_size=1/3, random_state=42, stratify=df['Level'])\n",
    "\n",
    "X_train_text = train_df['Cleaned_Body_n'].fillna('')\n",
    "X_test_text = test_df['Cleaned_Body_n'].fillna('')\n",
    "y_train = train_df['Level']\n",
    "y_test = test_df['Level']\n",
    "\n",
    "X_train_numeric = train_df[numeric_cols]\n",
    "X_test_numeric = test_df[numeric_cols]\n",
    "\n",
    "# 3 preprocessing: TF-IDF + scaling\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "# 4 pipeline\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# 5 combine with ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"tfidf\", tfidf, \"text\"),\n",
    "    (\"num\", numeric_transformer, numeric_cols)\n",
    "])\n",
    "\n",
    "X_train_all = train_df[numeric_cols].copy()\n",
    "X_test_all = test_df[numeric_cols].copy()\n",
    "X_train_all['text'] = X_train_text\n",
    "X_test_all['text'] = X_test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68042,
     "status": "ok",
     "timestamp": 1750080359149,
     "user": {
      "displayName": "Andrea Guarnieri",
      "userId": "03250951868602502178"
     },
     "user_tz": -120
    },
    "id": "3rp6p6I5ewDn",
    "outputId": "2129687e-d769-4d7b-ac34-7d12f0da6b22"
   },
   "outputs": [],
   "source": [
    "model_svm = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"clf\", LinearSVC(class_weight='balanced', random_state=42, max_iter=10000))\n",
    "])\n",
    "\n",
    "# label encoding del target\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "# compute weights\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train_enc)\n",
    "\n",
    "# fit with sample_weight\n",
    "model_svm.fit(X_train_all, y_train_enc, clf__sample_weight=sample_weights)\n",
    "\n",
    "y_pred_enc_svm = model_svm.predict(X_test_all)\n",
    "y_pred_svm = le.inverse_transform(y_pred_enc_svm)\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 573,
     "status": "ok",
     "timestamp": 1750080359719,
     "user": {
      "displayName": "Andrea Guarnieri",
      "userId": "03250951868602502178"
     },
     "user_tz": -120
    },
    "id": "GJABAbuOuCfL",
    "outputId": "0cedc027-6506-4f5a-b1b5-02b738938ef8"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Confusion matrix\n",
    "cm_email = confusion_matrix(y_test, y_pred_svm, labels=le.classes_)\n",
    "cm_email_norm = cm_email.astype('float') / cm_email.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    cm_email_norm,\n",
    "    annot=True,\n",
    "    fmt='.2%',\n",
    "    cmap='Blues',\n",
    "    xticklabels=le.classes_,\n",
    "    yticklabels=le.classes_\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix at email level (percentages per class)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "YU23NYF9x4Il",
    "SWfRloJZNy99",
    "LIXYmLxTx0UC"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
